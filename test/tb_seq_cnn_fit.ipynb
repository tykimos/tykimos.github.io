{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 객담도말 결핵진단 딥러닝 모델\n",
    "\n",
    "CNN 기반의 객담도말 결핵진단 딥러닝 모델 소스코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 모델관련 환경설정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class test():\n",
    "    \n",
    "    def __init__(self):        \n",
    "        self.a = 0\n",
    "        \n",
    "    def func1(self, a):\n",
    "        self.a = a\n",
    "        \n",
    "    def print_a(self):\n",
    "        print(self.a)\n",
    "\n",
    "test1 = test()\n",
    "test1.print_a()\n",
    "test1.func1(1)\n",
    "test1.print_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class printbatch(callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "#        if batch%10 == 0:\n",
    "            print \"Batch \" + str(batch) + \" ends\"\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        print(logs)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patch_channel        1\n",
      "num_patch_row            64\n",
      "num_patch_col            64\n",
      "num_sample_per_batch     32\n",
      "num_batch_per_cache      64\n",
      "num_sample_per_cache     2048\n",
      "sample_size              4096 (4.0KB)\n",
      "batch_size               131072 (128.0KB)\n",
      "cache_size               8388608 (8192.0KB)\n"
     ]
    }
   ],
   "source": [
    "# from insdeep.cacheprocessing import Parameter\n",
    "\n",
    "class Parameter():\n",
    "\n",
    "    def __init__(self, num_patch_channel, num_patch_row, num_patch_col, num_sample_per_batch, num_batch_per_cache):        \n",
    "        self.num_patch_channel = num_patch_channel # channel 수\n",
    "        self.num_patch_row = num_patch_row # row 수\n",
    "        self.num_patch_col = num_patch_col # col 수 \n",
    "        self.num_sample_per_batch = num_sample_per_batch # 한 배치 당 샘플 수\n",
    "        self.num_batch_per_cache = num_batch_per_cache # 한 캐쉬 당 배치 수\n",
    "        \n",
    "    def num_sample_per_batch(self):\n",
    "        return self.num_sample_per_batch\n",
    "    \n",
    "    def sample_size(self):\n",
    "        return self.num_patch_channel * self.num_patch_row * self.num_patch_col\n",
    "    \n",
    "    def batch_size(self):\n",
    "        return self.num_sample_per_batch * self.sample_size()\n",
    "    \n",
    "    def cache_size(self):\n",
    "        return self.num_batch_per_cache * self.batch_size()\n",
    "\n",
    "    def summary(self):\n",
    "        print('{:24s} {:d}'.format('num_patch_channel', self.num_patch_channel))\n",
    "        print('{:24s} {:d}'.format('num_patch_row', self.num_patch_row))\n",
    "        print('{:24s} {:d}'.format('num_patch_col', self.num_patch_col))\n",
    "        print('{:24s} {:d}'.format('num_sample_per_batch', self.num_sample_per_batch))\n",
    "        print('{:24s} {:d}'.format('num_batch_per_cache', self.num_batch_per_cache))\n",
    "        print('{:24s} {:d}'.format('num_sample_per_cache', self.num_sample_per_batch * self.num_batch_per_cache))\n",
    "        print('{:24s} {:d} ({:.1f}KB)'.format('sample_size', self.sample_size(), self.sample_size()/1024.0))\n",
    "        print('{:24s} {:d} ({:.1f}KB)'.format('batch_size', self.batch_size(), self.batch_size()/1024.0))\n",
    "        print('{:24s} {:d} ({:.1f}KB)'.format('cache_size', self.cache_size(), self.cache_size()/1024.0))\n",
    "        \n",
    "class Dataset():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 train_patch_filename, \n",
    "                 train_label_filename, \n",
    "                 validation_patch_filename,\n",
    "                 validation_label_filename):\n",
    "        \n",
    "        self.train_patch_fp = open(train_patch_filename, 'rb')\n",
    "        self.train_label_fp = open(train_label_filename, 'rb')\n",
    "        self.validation_patch_fp = open(validation_patch_filename, 'rb')\n",
    "        self.validation_label_fp = open(validation_label_filename, 'rb')\n",
    "\n",
    "    def close(self)\n",
    "        self.train_patch_fp.close()\n",
    "        self.train_label_fp.close()\n",
    "        self.validation_patch_fp.close()\n",
    "        self.validation_label_fp.close()\n",
    "        \n",
    "class Dispatcher():\n",
    "    \n",
    "    def __init__(self, parameter, dataset):\n",
    "        self.parameter = parameter\n",
    "        self.dataset = dateset\n",
    "        self.idx_cache = 0\n",
    "        self.idx_batch = 0\n",
    "        self.cache_buf_patch = []\n",
    "        self.cache_buf_label = []\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \n",
    "        start_idx_batch = \n",
    "        end_idx_batch = \n",
    "        \n",
    "        return self.cache_buf_patch[self.idx_batch*self.paramter.num_sample_per_batch():(self.idx_batch+1)*self.batch_size()], \n",
    "               self.cache_buf_label[self.idx_batch*BATCH_SIZE:(idx_batch+1)*BATCH_SIZE]\n",
    "\n",
    "NUM_PATCH_CHANNEL = 1\n",
    "NUM_PATCH_ROW = 64\n",
    "NUM_PATCH_COL = 64\n",
    "NUM_SAMPLE_PER_BATCH = 32\n",
    "NUM_BATCH_PER_CACHE = 64\n",
    "                \n",
    "cache_param = Parameter(NUM_PATCH_CHANNEL,\n",
    "                        NUM_PATCH_ROW,\n",
    "                        NUM_PATCH_COL,\n",
    "                        NUM_SAMPLE_PER_BATCH,\n",
    "                        NUM_BATCH_PER_CACHE)\n",
    "\n",
    "cache_param.summary()\n",
    "\n",
    "cache_dataset = Dataset('./datasets/train_image_64x64_gray_447648.bin', \n",
    "                        './datasets/train_label_64x64_gray_447648.bin',\n",
    "                        './datasets/train_image_64x64_gray_447648.bin', \n",
    "                        './datasets/train_label_64x64_gray_447648.bin')\n",
    "\n",
    "cache_dispatcher = Dispatcher(cache_param, cache_dataset)\n",
    "\n",
    "cache_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2 # 클래스 수\n",
    "NUM_EPOCHS = 2 # epoch 수\n",
    "NUM_FILTERS = 32 # convolution 필터 수\n",
    "NUM_POOL = 2 # max plling을 위한 pooling 영역 크기\n",
    "NUM_CONV = 3 # convolution 커널 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "NUM_SAMPLES = 447648\n",
    "BATCH_SIZE = 32 # 한 epoch에서 실행시키는 단위(배치)크기\n",
    "CACHE_BATCH_COUNT = 64\n",
    "\n",
    "sample_bytes = IMG_CHANNELS * IMG_ROWS * IMG_COLS\n",
    "batch_bytes = sample_bytes * BATCH_SIZE\n",
    "cache_bytes = batch_bytes * CACHE_BATCH_COUNT\n",
    "\n",
    "validation_num_samples = int(NUM_SAMPLES*1.0/4.0)\n",
    "train_num_samples = NUM_SAMPLES - validation_num_samples\n",
    "\n",
    "train_total_batch_count = train_num_samples / BATCH_SIZE\n",
    "train_total_cache_count = (train_total_batch_count + (CACHE_BATCH_COUNT-1)) / CACHE_BATCH_COUNT\n",
    "\n",
    "validation_total_batch_count = validation_num_samples / BATCH_SIZE\n",
    "validation_total_cache_count = (validation_total_batch_count + (CACHE_BATCH_COUNT-1)) / CACHE_BATCH_COUNT\n",
    "\n",
    "# index 기준은 batch 수\n",
    "print('A : 한 배치 당 패치 수\\t' + str(BATCH_SIZE))\n",
    "print('B : 한 캐쉬 당 배치 수\\t' + str(CACHE_BATCH_COUNT))\n",
    "print('C = A x B : 한 캐쉬 당 패치 수\\t' + str(BATCH_SIZE*CACHE_BATCH_COUNT))\n",
    "print('D : 한 패치 당 바이트 수\\t' + str(sample_bytes))\n",
    "print('E = A x D : 한 배치 당 바이트 수\\t' + str(batch_bytes))\n",
    "print('F = E x B : 한 캐쉬 당 바이트 수\\t' + str(cache_bytes) + ', ' + str(cache_bytes/1024) + 'KB')\n",
    "\n",
    "# train\n",
    "print('훈련 데이터 수\\t' + str(train_num_samples))\n",
    "print('훈련 배치 수\\t' + str(train_total_batch_count))\n",
    "print('훈련 캐쉬 수\\t' + str(train_total_cache_count))\n",
    "\n",
    "# validataion\n",
    "print('검증 데이터 수\\t' + str(validation_num_samples))\n",
    "print('검증 배치 수\\t' + str(validation_total_batch_count))\n",
    "print('검증 캐쉬 수\\t' + str(validation_total_cache_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 관련 환경설정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_FILE_PATH = './seq_model_cnn.h5'\n",
    "PREDICT_FILE_PATH = './predict.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img 자료 로딩 함수 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TrainDatasetGenerator(arg):\n",
    "    \n",
    "    print ('arg' + str(arg))\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    # x는 패치, y는 라벨\n",
    "    fp_x = open('./datasets/train_image_64x64_gray_447648.bin', 'rb')\n",
    "    fp_y = open('./datasets/train_label_64x64_gray_447648.bin', 'rb')\n",
    "\n",
    "    offset_start_x = 0\n",
    "    offset_start_y = 0\n",
    "    \n",
    "    while 1:\n",
    "        # for문 구조\n",
    "        # 전체 캐쉬수 만큼 루프\n",
    "        for idx_cache in range(train_total_cache_count):\n",
    "\n",
    "            # 해당 캐쉬 당 배치 수 계산 (마지막 캐쉬의 배치 수 계산 고려)\n",
    "            if ((idx_cache + 1) * CACHE_BATCH_COUNT) > train_total_batch_count:\n",
    "                batch_count = train_total_batch_count - idx_cache * CACHE_BATCH_COUNT\n",
    "            else:\n",
    "                batch_count = CACHE_BATCH_COUNT\n",
    "\n",
    "            # y인 경우에는 패치당 1바이트이므로 캐쉬당 패치수로 인덱스를 계산함            \n",
    "            offset_x = offset_start_x + idx_cache * cache_bytes\n",
    "            offset_y = offset_start_y + idx_cache * CACHE_BATCH_COUNT * BATCH_SIZE\n",
    "\n",
    "            # 현재 캐쉬에서 읽어야 할 바이트 수는 배치 수 x 배치 바이트 만큼임\n",
    "            read_cache_bytes = batch_bytes * batch_count\n",
    "\n",
    "            fp_x.seek(offset_x)\n",
    "            fp_y.seek(offset_y)\n",
    "\n",
    "            # 패치 읽음\n",
    "            # 파일에서 데이터를 읽어 캐쉬 버퍼에 로딩\n",
    "            cache_buf_x = fp_x.read(read_cache_bytes)\n",
    "            cache_data_x = np.frombuffer(cache_buf_x, dtype=np.uint8)\n",
    "            cache_data_x = cache_data_x.reshape(batch_count * BATCH_SIZE, IMG_CHANNELS, IMG_ROWS, IMG_COLS)\n",
    "            cache_data_x = cache_data_x.astype('float32')\n",
    "            cache_data_x /= 255\n",
    "\n",
    "            # 라벨 읽음\n",
    "            cache_buf_y = fp_y.read(batch_count * BATCH_SIZE)\n",
    "            cache_data_y = []\n",
    "            for i in cache_buf_y:\n",
    "                cache_data_y.append(i)\n",
    "            cache_data_y = np.asarray(cache_data_y, dtype=np.uint8, order='C')\n",
    "            # convert class vectors to binary class matrices\n",
    "            cache_data_y = np_utils.to_categorical(cache_data_y, NUM_CLASSES)\n",
    "\n",
    "            # 로딩한 캐쉬에서 배치 수 만큼 루프돌려 해당 배치를 넘겨줌\n",
    "            for idx_batch in range(batch_count):\n",
    "\n",
    "                print (count)\n",
    "                count = count + 1\n",
    "\n",
    "                yield cache_data_x[idx_batch*BATCH_SIZE:(idx_batch+1)*BATCH_SIZE], cache_data_y[idx_batch*BATCH_SIZE:(idx_batch+1)*BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ValidationDatasetGenerator(arg):\n",
    "    \n",
    "    print ('arg' + str(arg))\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    # x는 패치, y는 라벨\n",
    "    fp_x = open('./datasets/train_image_64x64_gray_447648.bin', 'rb')\n",
    "    fp_y = open('./datasets/train_label_64x64_gray_447648.bin', 'rb')\n",
    "\n",
    "    offset_start_x = train_num_samples * sample_bytes\n",
    "    offset_start_y = train_num_samples\n",
    "    \n",
    "    while 1:\n",
    "        # for문 구조\n",
    "        # 전체 캐쉬수 만큼 루프\n",
    "        for idx_cache in range(train_total_cache_count):\n",
    "\n",
    "            # 해당 캐쉬 당 배치 수 계산 (마지막 캐쉬의 배치 수 계산 고려)        \n",
    "            if ((idx_cache + 1) * CACHE_BATCH_COUNT) > validation_total_batch_count:\n",
    "                batch_count = validation_total_batch_count - idx_cache * CACHE_BATCH_COUNT\n",
    "            else:\n",
    "                batch_count = CACHE_BATCH_COUNT\n",
    "\n",
    "            # y인 경우에는 패치당 1바이트이므로 캐쉬당 패치수로 인덱스를 계산함            \n",
    "            offset_x = offset_start_x + idx_cache * cache_bytes\n",
    "            offset_y = offset_start_y + idx_cache * CACHE_BATCH_COUNT * BATCH_SIZE\n",
    "\n",
    "            # 현재 캐쉬에서 읽어야 할 바이트 수는 배치 수 x 배치 바이트 만큼임        \n",
    "            read_cache_bytes = batch_bytes * batch_count\n",
    "\n",
    "            fp_x.seek(offset_x)\n",
    "            fp_y.seek(offset_y)\n",
    "\n",
    "            # 패치 읽음\n",
    "            # 파일에서 데이터를 읽어 캐쉬 버퍼에 로딩\n",
    "            cache_buf_x = fp_x.read(read_cache_bytes)\n",
    "            cache_data_x = np.frombuffer(cache_buf_x, dtype=np.uint8)\n",
    "            cache_data_x = cache_data_x.reshape(batch_count * BATCH_SIZE, IMG_CHANNELS, IMG_ROWS, IMG_COLS)\n",
    "            cache_data_x = cache_data_x.astype('float32')\n",
    "            cache_data_x /= 255\n",
    "\n",
    "            # 라벨 읽음        \n",
    "            cache_buf_y = fp_y.read(batch_count * BATCH_SIZE)\n",
    "            cache_data_y = []\n",
    "            for i in cache_buf_y:\n",
    "                cache_data_y.append(i)\n",
    "            cache_data_y = np.asarray(cache_data_y, dtype=np.uint8, order='C')\n",
    "            # convert class vectors to binary class matrices\n",
    "            cache_data_y = np_utils.to_categorical(cache_data_y, NUM_CLASSES)\n",
    "\n",
    "            # 로딩한 캐쉬에서 배치 수 만큼 루프돌려 해당 배치를 넘겨줌        \n",
    "            for idx_batch in range(batch_count):\n",
    "\n",
    "                print (count)\n",
    "                count = count + 1\n",
    "\n",
    "                yield cache_data_x[idx_batch*BATCH_SIZE:(idx_batch+1)*BATCH_SIZE], cache_data_y[idx_batch*BATCH_SIZE:(idx_batch+1)*BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥리닝 모델을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    " \n",
    "model.add(Convolution2D(NUM_FILTERS, NUM_CONV, NUM_CONV,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=(IMG_CHANNELS, IMG_ROWS, IMG_COLS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(NUM_FILTERS, NUM_CONV, NUM_CONV))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(NUM_POOL, NUM_POOL)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# np_utils.visualize_util.plot(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 모델을 구축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pb = printbatch()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.fit(train_img, \n",
    "#          train_label, \n",
    "#          batch_size=BATCH_SIZE, \n",
    "#          nb_epoch=NUM_EPOCHS,\n",
    "#          verbose=1, \n",
    "#          validation_data=(validation_img, validation_label))\n",
    "\n",
    "model.fit_generator(TrainDatasetGenerator(1), \n",
    "                    samples_per_epoch = train_num_samples,                     \n",
    "                    nb_epoch = NUM_EPOCHS, \n",
    "                    verbose=1, \n",
    "                    #callbacks=None,#[pb], \n",
    "                    validation_data=ValidationDatasetGenerator(2),\n",
    "                    nb_val_samples = validation_num_samples,\n",
    "                    class_weight=None, \n",
    "                    max_q_size = 10,\n",
    "                    nb_worker=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 모델 테스트를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(test_img, test_label, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = model.predict_classes(test_img, batch_size=32)\n",
    "np.savetxt(PREDICT_FILE_PATH, classes, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.save_weights(MODEL_SAVE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
