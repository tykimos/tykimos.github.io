---
layout: post
title: "ì†ŒìŠ¤ì½”ë“œ ìƒì„± ì „ìš© - CodeGemma ì‹œì‘í•˜ê¸°"
author: Taeyoung Kim
date: 2024-4-10 14:56:03
categories: llm gemma codegemma
comments: true
image: http://tykimos.github.io/warehouse/2024/2024-4-10-getting_started_with_codegemma_title.jpg
---

![img](http://tykimos.github.io/warehouse/2024/2024-4-10-getting_started_with_codegemma_title.jpg)

CodeGemmaëŠ” Gemini ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì œì‘ë˜ì–´, ê²½ëŸ‰í™”ë˜ì—ˆìœ¼ë©° ê¸°ê¸° ë‚´ì—ì„œë„ ì‘ë™ ê°€ëŠ¥í•œ ëª¨ë¸ì…ë‹ˆë‹¤. ì£¼ë¡œ ì½”ë“œ ê´€ë ¨ 5000ì–µ ê°œ ì´ìƒì˜ í† í°ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë©°, Gemma ëª¨ë¸ ê³„ì—´ê³¼ ë™ì¼í•œ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ì— ë”°ë¼, CodeGemma ëª¨ë¸ì€ ì½”ë“œ ì™„ì„± ë° ìƒì„± ì‘ì—… ëª¨ë‘ì—ì„œ ìµœì‹  ê¸°ìˆ ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ë©´ì„œë„, ë‹¤ì–‘í•œ ê·œëª¨ì—ì„œ ê°•ë ¥í•œ ì´í•´ë ¥ê³¼ ì¶”ë¡  ëŠ¥ë ¥ì„ ìœ ì§€í•œë‹¤ê³  í•©ë‹ˆë‹¤.

CodeGemmaëŠ” ë‹¤ìŒê³¼ ê°™ì€ 3ê°€ì§€ ë²„ì „ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤:

- 7B ì½”ë“œ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸
- 7B ì§€ì‹œ ê¸°ë°˜ íŠœë‹ ì½”ë“œ ëª¨ë¸
- ì½”ë“œ ì±„ìš°ê¸° ë° ê°œë°©í˜• ìƒì„±ì„ ìœ„í•´ íŠ¹ë³„íˆ í›ˆë ¨ëœ 2B ëª¨ë¸.

ì´ ê°€ì´ë“œì—ì„œëŠ” KerasNLPë¥¼ í™œìš©í•˜ì—¬ CodeGemma 2B ëª¨ë¸ë¡œ ì½”ë“œ ì™„ì„± ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•ˆë‚´í•©ë‹ˆë‹¤. ì¶”ê°€ë¡œ êµ¬êµ¬ë‹¨ ì˜ˆì œë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  ëª¨ë¸ì˜ ì¶œë ¥ì—ì„œ ì™„ì„±ëœ ì½”ë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒê¹Œì§€ í•´ë³´ê² ìŠµë‹ˆë‹¤.

ë³¸ ê²Œì‹œë¬¼ì€ ì•„ë˜ ë§í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. (Copyright 2024 Google LLC.)

- https://ai.google.dev/gemma/docs/codegemma/keras_quickstart

### í•¨ê»˜ë³´ê¸°

* 1í¸ - Gemma ì‹œì‘í•˜ê¸° ë¹ ë¥¸ì‹¤í–‰ (ì¶”í›„ ê³µê°œ)
* [2í¸ - Gemma LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_lora_fine_tuning_fast_execute/)
* [3í¸ - Gemma í•œêµ­ì–´ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_korean_lora_fine_tuning_fast_execute/)
* [4í¸ - Gemma ì˜í•œë²ˆì—­ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_en2ko_lora_fine_tuning_fast_execute/)
* [5í¸ - Gemma í•œì˜ë²ˆì—­ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_ko2en_lora_fine_tuning_fast_execute/)
* [6í¸ - Gemma í•œêµ­ì–´ SQLì±—ë´‡ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/23/gemma_ko2sql_lora_fine_tuning_fast_execute/)
* [7í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì›¹ë¸Œë¼ìš°ì €í¸ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/04/02/gemma_ondevice_webbrowser_fast_execute/)
* [8í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì•„ì´í°(iOS)í¸ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/04/05/local_llm_installed_on_my_iphone_gemma_2b/)
* 9í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì•ˆë“œë¡œì´ë“œí¸ ë¹ ë¥¸ì‹¤í–‰ (ì‘ì—…ì¤‘)
* [10í¸ - RLHF íŠœë‹ìœ¼ë¡œ í–¥ìƒëœ Gemma 1.1 2B IT ê³µê°œ](https://tykimos.github.io/2024/04/08/rlhf_tuning_enhanced_gemma_1.1_2b_it_release/)
* [11í¸ - ì†ŒìŠ¤ì½”ë“œ ìƒì„± ì „ìš© - CodeGemma ì‹œì‘í•˜ê¸°](https://tykimos.github.io/2024/04/10/getting_started_with_codegemma/)


```
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```

### CodeGemma ì ‘ê·¼ê¶Œí•œ ì–»ê¸°

ë¨¼ì € Gemma ì„¤ì •ì—ì„œ ì„¤ì • ì§€ì¹¨ì„ ì™„ë£Œí•´ì•¼ í•©ë‹ˆë‹¤. Gemma ì„¤ì • ì§€ì¹¨ì€ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

- [ìºê¸€](kaggle.com)ì„ ê°€ì…í•©ë‹ˆë‹¤.
- https://www.kaggle.com/models/google/gemma ì— ì ‘ì†í•˜ì—¬ ê¶Œí•œ ìš”ì²­ì„ í•©ë‹ˆë‹¤.
- Kaggle ì‚¬ìš©ì ì´ë¦„ê³¼ API í‚¤ë¥¼ ìƒì„±í•˜ê³  êµ¬ì„±í•˜ê¸° ìœ„í•´ Kaggle ì‚¬ìš©ì í”„ë¡œí•„ì˜ ê³„ì • íƒ­ìœ¼ë¡œ ì´ë™í•˜ì—¬ ìƒˆ í† í° ìƒì„±ì„ ì„ íƒí•©ë‹ˆë‹¤.
- API ìê²© ì¦ëª…ì´ í¬í•¨ëœ kaggle.json íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


### ì½”ë© í™˜ê²½ ì„¤ì •

CodeGemma 2B ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ê°–ì¶˜ Colab ëŸ°íƒ€ì„ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ ê²½ìš° T4 GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ë©”ë‰´** - **ëŸ°íƒ€ì„** - **ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½**ì„ ì„ íƒí•©ë‹ˆë‹¤.
- í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ì—ì„œ **T4 GPU**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

ë‹¤ìŒì€ API í‚¤ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

- ë‹¤ìš´ë¡œë“œ í•œ kaggle.jsonì„ ì—´ì–´ë³´ë©´, Kaggle ì‚¬ìš©ì ì´ë¦„ê³¼ Kaggle API ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
- ì½”ë©ì—ì„œ ì™¼ìª½ íŒ¨ë„ì˜ ë³´ì•ˆë¹„ë°€(ğŸ”‘)ì„ ì„ íƒí•˜ê³  í‚¤ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
- Kaggle ì‚¬ìš©ì ì´ë¦„ì€ KAGGLE_USERNAMEìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
- Kaggle APIëŠ” KAGGLE_KEYë¡œ ì €ì¥í•©ë‹ˆë‹¤.

### í™˜ê²½ë³€ìˆ˜ ì„¤ì •í•˜ê¸°

ìœ„ì—ì„œ ì €ì¥í•œ `KAGGLE_USERNAME`ê³¼ `KAGGLE_KEY`ì„ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.


```
import os
from google.colab import userdata

os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')
os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')
```

### í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜


```
!pip install -q -U keras-nlp
```

    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m508.4/508.4 kB[0m [31m7.5 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m950.8/950.8 kB[0m [31m42.3 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m5.2/5.2 MB[0m [31m23.7 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m589.8/589.8 MB[0m [31m2.6 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m4.8/4.8 MB[0m [31m98.2 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m2.2/2.2 MB[0m [31m88.1 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m5.5/5.5 MB[0m [31m96.2 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.1/1.1 MB[0m [31m67.8 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m311.2/311.2 kB[0m [31m30.6 MB/s[0m eta [36m0:00:00[0m
    [?25h[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.[0m[31m
    [0m

### ë°±ì—”ë“œ ì„ íƒ

ì¼€ë¼ìŠ¤ëŠ” TensorFlow, JAX, ë˜ëŠ” PyTorch ì¤‘ í•˜ë‚˜ë¥¼ ë°±ì—”ë“œë¡œ ì„ íƒí•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì˜ˆì œì—ì„œëŠ” TensorFlowë¥¼ ë°±ì—”ë“œë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.


```
os.environ["KERAS_BACKEND"] = "tensorflow"  # Or "jax" or "torch".
```

### íŒ¨í‚¤ì§€ ê°€ì ¸ì˜¤ê¸°

í•„ìš”í•œ íŒ¨í‚¤ì§€ëŠ” Kerasì™€ KerasNLP ì…ë‹ˆë‹¤.


```
import keras_nlp
import keras

# Run at half precision.
keras.config.set_floatx("bfloat16")
```

### ëª¨ë¸ ë¡œë“œ

ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸(causal language modeling)ì„ ìœ„í•œ ì—”ë“œ íˆ¬ ì—”ë“œ Gamma ëª¨ë¸ì¸ `GemmaCausalLM`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. `from_preset`ì„ ì´ìš©í•´ì„œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤. `from_preset` í•¨ìˆ˜ëŠ” ì‚¬ì „ ì„¤ì •ëœ ì•„í‚¤í…ì²˜ì™€ ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤. ìœ„ ì½”ë“œì—ì„œ ë¬¸ìì—´ code_gemma_2b_enì€ ì‚¬ì „ ì„¤ì • ì•„í‚¤í…ì²˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. 20ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ CodeGemma ëª¨ë¸ì…ë‹ˆë‹¤. 70ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ CodeGemma 7B ëª¨ë¸ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ Colabì—ì„œ ë” í° ëª¨ë¸ì„ ì‹¤í–‰í•˜ë ¤ë©´ ìœ ë£Œ í”Œëœì—ì„œ ì œê³µí•˜ëŠ” í”„ë¦¬ë¯¸ì—„ GPUì— ëŒ€í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.


```
# code_gemma_2b_en ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("code_gemma_2b_en")
gemma_lm.summary()
```


<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Preprocessor: "gemma_causal_lm_preprocessor_1"</span>
</pre>




<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ<span style="font-weight: bold"> Tokenizer (type)                                   </span>â”ƒ<span style="font-weight: bold">                                             Vocab # </span>â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ gemma_tokenizer (<span style="color: #0087ff; text-decoration-color: #0087ff">GemmaTokenizer</span>)                   â”‚                                             <span style="color: #00af00; text-decoration-color: #00af00">256,000</span> â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>




<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "gemma_causal_lm_1"</span>
</pre>




<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ<span style="font-weight: bold"> Layer (type)                  </span>â”ƒ<span style="font-weight: bold"> Output Shape              </span>â”ƒ<span style="font-weight: bold">         Param # </span>â”ƒ<span style="font-weight: bold"> Connected to               </span>â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ padding_mask (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)     â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)              â”‚               <span style="color: #00af00; text-decoration-color: #00af00">0</span> â”‚ -                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ token_ids (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)        â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>)              â”‚               <span style="color: #00af00; text-decoration-color: #00af00">0</span> â”‚ -                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gemma_backbone                â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">2048</span>)        â”‚   <span style="color: #00af00; text-decoration-color: #00af00">2,506,172,416</span> â”‚ padding_mask[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>],        â”‚
â”‚ (<span style="color: #0087ff; text-decoration-color: #0087ff">GemmaBackbone</span>)               â”‚                           â”‚                 â”‚ token_ids[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ token_embedding               â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">256000</span>)      â”‚     <span style="color: #00af00; text-decoration-color: #00af00">524,288,000</span> â”‚ gemma_backbone[<span style="color: #00af00; text-decoration-color: #00af00">0</span>][<span style="color: #00af00; text-decoration-color: #00af00">0</span>]       â”‚
â”‚ (<span style="color: #0087ff; text-decoration-color: #0087ff">ReversibleEmbedding</span>)         â”‚                           â”‚                 â”‚                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>




<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,506,172,416</span> (4.67 GB)
</pre>




<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">2,506,172,416</span> (4.67 GB)
</pre>




<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>



### ì¤‘ê°„ ì±„ìš°ê¸° ì½”ë“œ ì™„ì„± íƒœìŠ¤í¬

ë³¸ ì˜ˆì œëŠ” CodeGemmaì˜ ì¤‘ê°„ ì±„ìš°ê¸°(Fill-in-the-Middle, FIM) ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œë¥¼ ì™„ì„±í•©ë‹ˆë‹¤. ì´ëŠ” ì½”ë“œ ì—ë””í„° ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì»¤ì„œ ì „í›„ì˜ ì½”ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì»¤ì„œ ìœ„ì¹˜ì— ì½”ë“œë¥¼ ì‚½ì…í•˜ëŠ” ë° íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤. CodeGemmaëŠ” 4ê°œì˜ ì‚¬ìš©ì ì •ì˜ í† í°ì„ ì‚¬ìš©í•˜ë„ë¡ í—ˆìš©í•©ë‹ˆë‹¤ - FIMì„ ìœ„í•œ 3ê°œì™€ ë‹¤ì¤‘ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì§€ì›ì„ ìœ„í•œ í† í°ì…ë‹ˆë‹¤.


```
BEFORE_CURSOR = "<|fim_prefix|>"
AFTER_CURSOR = "<|fim_suffix|>"
AT_CURSOR = "<|fim_middle|>"
FILE_SEPARATOR = "<|file_separator|>"
```

ëª¨ë¸ì„ ìœ„í•œ ì •ì§€ í† í°ì„ ì •ì˜í•©ë‹ˆë‹¤.



```
END_TOKEN = gemma_lm.preprocessor.tokenizer.end_token

stop_tokens = (BEFORE_CURSOR, AFTER_CURSOR, AT_CURSOR, FILE_SEPARATOR, END_TOKEN)

stop_token_ids = tuple(gemma_lm.preprocessor.tokenizer.token_to_id(x) for x in stop_tokens)
```

ì½”ë“œ ì™„ì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í¬ë§·ì„ ì•„ë˜ ê°€ì´ë“œì— ë”°ë¼ ì •ì˜í•©ë‹ˆë‹¤.

- FIM í† í°ê³¼ ì ‘ë‘ì‚¬ ë° ì ‘ë¯¸ì‚¬ ì‚¬ì´ì—ëŠ” ê³µë°±ì´ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤.
- ëª¨ë¸ì´ ì±„ìš°ê¸°ë¥¼ ê³„ì†í•˜ë„ë¡ ìœ ë„í•˜ê¸° ìœ„í•´ FIM ì¤‘ê°„ í† í°ì€ ëì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
- í˜„ì¬ íŒŒì¼ì—ì„œ ì»¤ì„œì˜ ìœ„ì¹˜ë‚˜ ëª¨ë¸ì— ì œê³µí•˜ê³ ì í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ì˜ ì–‘ì— ë”°ë¼ ì ‘ë‘ì‚¬ë‚˜ ì ‘ë¯¸ì‚¬ëŠ” ë¹„ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì„ ìœ„í•œ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” before ì½”ë“œì™€ after ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ë©´, ëª¨ë¸ì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•´ì¤ë‹ˆë‹¤.


```
def format_completion_prompt(before, after):
    return f"{BEFORE_CURSOR}{before}{AFTER_CURSOR}{after}{AT_CURSOR}"

before = "import "
after = """if __name__ == "__main__":\n    sys.exit(0)"""
prompt = format_completion_prompt(before, after)
print(prompt)
```

    <|fim_prefix|>import <|fim_suffix|>if __name__ == "__main__":
        sys.exit(0)<|fim_middle|>


ì¦‰ ì•„ë˜ì²˜ëŸ¼ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

- fim_prefix
- ì´ì „ ì½”ë“œ
- fim_suffix
- ì´í›„ ì½”ë“œ
- fim_middle

Run the prompt. It is recommended to stream response tokens. Stop streaming upon encountering any of the user-defined or end of turn/senetence tokens to get the resulting code completion.

ê·¸ ë‹¤ìŒì€ í”„ë¡¬í”„íŠ¸ë¥¼ gemma ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì‹¤í–‰ì‹œí‚µë‹ˆë‹¤. stop_token_idsì„ ì •ì˜í•˜ì—¬ ì •ì§€ í† í°ì´ë‚˜ ë¬¸ì¥ì˜ ëì„ ê°ì§€í•˜ë©´ í•´ë‹¹ ë¶€ë¶„ì—ì„œ ìƒì„±ì„ ì¤‘ë‹¨í•˜ë„ë¡ í•©ë‹ˆë‹¤.


```
gemma_lm.generate(prompt, stop_token_ids=stop_token_ids, max_length=128)
```




    '<|fim_prefix|>import <|fim_suffix|>if __name__ == "__main__":\n    sys.exit(0)<|fim_middle|>sys\n<|file_separator|>'



ëª¨ë¸ì´ ì½”ë“œ ì™„ì„±ì„ ìœ„í•´ `sys`ì„ ì œì•ˆí•´ì¤ë‹ˆë‹¤.

### ì¶”ê°€ ì˜ˆì œ

êµ¬êµ¬ë‹¨ ì½”ë“œë¥¼ ì™„ì„±í•˜ëŠ” ì˜ˆì œë¥¼ í…ŒìŠ¤íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤. êµ¬êµ¬ë‹¨ ì¼ë¶€ ì½”ë“œë¥¼ ì…ë ¥í•˜ì—¬ ì¤‘ê°„ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.


```
before = """
def print_multiplication_table():
    for i in range(1, 10):
"""

after = """
if __name__ == "__main__":
    print_multiplication_table()
"""
prompt = format_completion_prompt(before, after)
response = gemma_lm.generate(prompt, stop_token_ids=stop_token_ids, max_length=128)

print(response)
```

    <|fim_prefix|>
    def print_multiplication_table():
        for i in range(1, 10):
    <|fim_suffix|>
    if __name__ == "__main__":
        print_multiplication_table()
    <|fim_middle|>        for j in range(1, 10):
                print(f"{i} x {j} = {i * j}")
            print()<|file_separator|>


ëª¨ë¸ì˜ ì¶œë ¥ì—ì„œ ì¤‘ê°„ ì½”ë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.


```
def extract_inner_code(code, start_token =  "<|fim_middle|>", end_token = "<|file_separator|>"):
    """
    ì£¼ì–´ì§„ ì½”ë“œì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì‚¬ì´ì˜ ë‚´ë¶€ ì½”ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
    - code (str): ë¶„ì„í•  ì „ì²´ ì½”ë“œ ë¬¸ìì—´
    - start_token (str): ë‚´ë¶€ ì½”ë“œì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” í† í°
    - end_token (str): ë‚´ë¶€ ì½”ë“œì˜ ì¢…ë£Œë¥¼ ë‚˜íƒ€ë‚´ëŠ” í† í°

    Returns:
    - str: ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì‚¬ì´ì— ìœ„ì¹˜í•œ ë‚´ë¶€ ì½”ë“œ ë¬¸ìì—´
    """
    start_index = code.find(start_token) + len(start_token)
    end_index = code.find(end_token, start_index)
    inner_code = code[start_index:end_index]

    return inner_code
```

extract_inner_code() í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì¤‘ê°„ì½”ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.


```
inner_code = extract_inner_code(response)
```


```
print(inner_code)
```

            for j in range(1, 10):
                print(f"{i} x {j} = {i * j}")
            print()


ì „ì²´ ì½”ë“œë¥¼ êµ¬ì„±í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.


```
full_code = before + inner_code + after
```


```
print(full_code)
```

    
    def print_multiplication_table():
        for i in range(1, 10):
            for j in range(1, 10):
                print(f"{i} x {j} = {i * j}")
            print()
    if __name__ == "__main__":
        print_multiplication_table()
    


### ë§ˆë¬´ë¦¬

CodeGemmaë¥¼ ì´ìš©í•˜ì—¬ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œë¥¼ ì±„ìš°ëŠ” ë°©ë²•ì— ëŒ€í•´ ì‚´í´ë´¤ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë§í¬ë¥¼ í•¨ê»˜ ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.

- [AI Assisted Programming with CodeGemma and KerasNLP notebook](https://ai.google.dev/gemma/docs/codegemma/code_assist_keras)
- [CodeGemma model card](https://ai.google.dev/gemma/docs/codegemma/model_card)

### ì¶”ê°€ë¬¸ì˜

* ì‘ì„±ì : ê¹€íƒœì˜
* ì´ë©”ì¼ : tykim@aifactory.page

### í•¨ê»˜ë³´ê¸°

* 1í¸ - Gemma ì‹œì‘í•˜ê¸° ë¹ ë¥¸ì‹¤í–‰ (ì¶”í›„ ê³µê°œ)
* [2í¸ - Gemma LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_lora_fine_tuning_fast_execute/)
* [3í¸ - Gemma í•œêµ­ì–´ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_korean_lora_fine_tuning_fast_execute/)
* [4í¸ - Gemma ì˜í•œë²ˆì—­ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_en2ko_lora_fine_tuning_fast_execute/)
* [5í¸ - Gemma í•œì˜ë²ˆì—­ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_ko2en_lora_fine_tuning_fast_execute/)
* [6í¸ - Gemma í•œêµ­ì–´ SQLì±—ë´‡ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/23/gemma_ko2sql_lora_fine_tuning_fast_execute/)
* [7í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì›¹ë¸Œë¼ìš°ì €í¸ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/04/02/gemma_ondevice_webbrowser_fast_execute/)
* [8í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì•„ì´í°(iOS)í¸ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/04/05/local_llm_installed_on_my_iphone_gemma_2b/)
* 9í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì•ˆë“œë¡œì´ë“œí¸ ë¹ ë¥¸ì‹¤í–‰ (ì‘ì—…ì¤‘)
* [10í¸ - RLHF íŠœë‹ìœ¼ë¡œ í–¥ìƒëœ Gemma 1.1 2B IT ê³µê°œ](https://tykimos.github.io/2024/04/08/rlhf_tuning_enhanced_gemma_1.1_2b_it_release/)
* [11í¸ - ì†ŒìŠ¤ì½”ë“œ ìƒì„± ì „ìš© - CodeGemma ì‹œì‘í•˜ê¸°](https://tykimos.github.io/2024/04/10/getting_started_with_codegemma/)