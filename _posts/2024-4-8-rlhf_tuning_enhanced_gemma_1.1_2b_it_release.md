---
layout: post
title: "RLHF íŠœë‹ìœ¼ë¡œ í–¥ìƒëœ Gemma 1.1 2B IT ê³µê°œ"
author: Taeyoung Kim
date: 2024-4-8 01:53:12
categories: llm, gemma
comments: true
image: http://tykimos.github.io/warehouse/2024/2024-4-8-rlhf_tuning_enhanced_gemma_1.1_2b_it_release_title.jpg
---

![img](http://tykimos.github.io/warehouse/2024/2024-4-8-rlhf_tuning_enhanced_gemma_1.1_2b_it_release_title.jpg)

### í•¨ê»˜ë³´ê¸°

* 1í¸ - Gemma ì‹œì‘í•˜ê¸° ë¹ ë¥¸ì‹¤í–‰ (ì¶”í›„ ê³µê°œ)
* [2í¸ - Gemma LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_lora_fine_tuning_fast_execute/)
* [3í¸ - Gemma í•œêµ­ì–´ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_korean_lora_fine_tuning_fast_execute/)
* [4í¸ - Gemma ì˜í•œë²ˆì—­ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_en2ko_lora_fine_tuning_fast_execute/)
* [5í¸ - Gemma í•œì˜ë²ˆì—­ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_ko2en_lora_fine_tuning_fast_execute/)
* [6í¸ - Gemma í•œêµ­ì–´ SQLì±—ë´‡ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/23/gemma_ko2sql_lora_fine_tuning_fast_execute/)
* [7í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì›¹ë¸Œë¼ìš°ì €í¸ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/04/02/gemma_ondevice_webbrowser_fast_execute/)
* [8í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì•„ì´í°(iOS)í¸ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/04/05/local_llm_installed_on_my_iphone_gemma_2b/)
* 9í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì•ˆë“œë¡œì´ë“œí¸ ë¹ ë¥¸ì‹¤í–‰ (ì‘ì—…ì¤‘)
* [10í¸ - RLHF íŠœë‹ìœ¼ë¡œ í–¥ìƒëœ Gemma 1.1 2B IT ê³µê°œ](https://tykimos.github.io/2024/04/08/rlhf_tuning_enhanced_gemma_1.1_2b_it_release/)

ì´ ë¬¸ì„œëŠ” https://huggingface.co/google/gemma-1.1-2b-it ê¸°ë°˜ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

## í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

ì•„ë˜ 3ê°œì˜ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. ë§Œì•½ì— ê²½ê³  ë©”ì‹œì§€ì— ëŸ°íƒ€ì„ ì¬ì‹œì‘í•´ì•¼í•œë‹¤ëŠ” ë©”ì‹œì§€ê°€ ë‚˜ì˜¨ë‹¤ë©´ [ë©”ë‰´] > [ëŸ°íƒ€ì„] > [ì„¸ì…˜ ë‹¤ì‹œ ì‹œì‘]ì„ í´ë¦­í•˜ì—¬ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•©ë‹ˆë‹¤.


```python
!pip install torch          # PyTorch, ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
!pip install transformers   # Hugging Faceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
!pip install accelerate     # Hugging Faceì˜ Accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
```

    Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)
    Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)
    Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)
    Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)
    Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)
    Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)
    Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)
      Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m23.7/23.7 MB[0m [31m38.5 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)
      Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m823.6/823.6 kB[0m [31m58.7 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)
      Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m14.1/14.1 MB[0m [31m48.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)
      Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m731.7/731.7 MB[0m [31m1.0 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)
      Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m410.6/410.6 MB[0m [31m2.5 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)
      Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m121.6/121.6 MB[0m [31m5.7 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)
      Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m56.5/56.5 MB[0m [31m7.5 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)
      Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m124.2/124.2 MB[0m [31m7.2 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)
      Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m196.0/196.0 MB[0m [31m2.4 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)
      Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m166.0/166.0 MB[0m [31m5.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)
      Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m99.1/99.1 kB[0m [31m14.7 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)
    Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)
      Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m21.1/21.1 MB[0m [31m18.3 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)
    Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)
    Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12
    Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105
    Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)
    Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)
    Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)
    Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)
    Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)
    Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)
    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)
    Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)
    Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)
    Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)
    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)
    Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)
    Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)
    Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)
    Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)
    Collecting accelerate
      Downloading accelerate-0.29.1-py3-none-any.whl (297 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m297.3/297.3 kB[0m [31m2.8 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)
    Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)
    Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)
    Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)
    Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)
    Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)
    Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)
    Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.3)
    Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)
    Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)
    Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)
    Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)
    Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)
    Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)
    Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)
    Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)
    Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)
    Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)
    Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)
    Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)
    Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)
    Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)
    Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)
    Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)
    Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)
    Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)
    Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)
    Installing collected packages: accelerate
    Successfully installed accelerate-0.29.1
    Collecting qrcode
      Downloading qrcode-7.4.2-py3-none-any.whl (46 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m46.2/46.2 kB[0m [31m895.7 kB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qrcode) (4.10.0)
    Collecting pypng (from qrcode)
      Downloading pypng-0.20220715.0-py3-none-any.whl (58 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m58.1/58.1 kB[0m [31m3.2 MB/s[0m eta [36m0:00:00[0m
    [?25hInstalling collected packages: pypng, qrcode
    Successfully installed pypng-0.20220715.0 qrcode-7.4.2


## í—ˆê¹…í˜ì´ìŠ¤ ì ‘ê·¼ í† í° ë°œí–‰

Hugging Face ì¸ì¦ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. 'HF_TOKEN'ì€ Hugging Face í† í°ì„ ì €ì¥í•˜ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•œ ì‚¬ìš©ì íŠ¹ì • í™˜ê²½ ë³€ìˆ˜ì…ë‹ˆë‹¤.

![colab user data key setting](http://tykimos.github.io/warehouse/2024/2024-4-8-rlhf_tuning_enhanced_gemma_1.1_2b_it_release_3.png)

## í—ˆê¹…í˜ì´ìŠ¤ ì ‘ê·¼ í† í° ì„¤ì •

í—ˆê¹…í˜ì´ìŠ¤ ì ‘ê·¼ í† í°ì„ ë°œí–‰í•˜ì˜€ë‹¤ë©´ 'HF_TOKEN'ì´ë¦„ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ë¡œ ë“±ë¡í•©ë‹ˆë‹¤. ì½”ë©ì—ì„œ ì¢€ ë” ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì½”ë©ì˜ "ë³´ì•ˆ ë¹„ë°€" ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ í‚¤ ê°’ì„ ê°€ì§€ê³  ì˜µë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì†ŒìŠ¤ì½”ë“œ ê³µìœ  ì‹œì— í‚¤ë¥¼ ê³µìœ í•˜ì§€ ì•Šì•„ë„ ë˜ë©°, ë‹¤ë¥¸ ì½”ë© ì†ŒìŠ¤ì½”ë“œ ì‚¬ìš©í•  ë•Œë„ ì¬ì‚¬ìš©ì´ ìš©ì´í•©ë‹ˆë‹¤.

![colab user data key setting](http://tykimos.github.io/warehouse/2024/2024-4-8-rlhf_tuning_enhanced_gemma_1.1_2b_it_release_2.png)

```python
import os
from google.colab import userdata

os.environ["HF_TOKEN"] = userdata.get('HF_TOKEN')
```

## ëª¨ë¸ ì¤€ë¹„í•˜ê¸°

"google/gemma-1.1-2b-it"ì— ëŒ€í•œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ì¤€ë¹„í•©ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ í•„ìš”í•œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤. ê°€ì¥ í° íŒŒì¼ì´ 4.95G ì •ë„ ë˜ë„¤ìš”.

![downloading from huggingface](http://tykimos.github.io/warehouse/2024/2024-4-8-rlhf_tuning_enhanced_gemma_1.1_2b_it_release_1.png)


```python
# Hugging Faceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Hugging Face ëª¨ë¸ ì €ì¥ì†Œì—ì„œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
# ëª¨ë¸ ì‹ë³„ì 'google/gemma-1.1-2b-it'ëŠ” ë¡œë“œí•  ëª¨ë¸ì„ ì§€ì •í•©ë‹ˆë‹¤.
tokenizer = AutoTokenizer.from_pretrained("google/gemma-1.1-2b-it")
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-1.1-2b-it",   # ëª¨ë¸ ì‹ë³„ì
    device_map="auto",          # ëª¨ë¸ì„ ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜(GPU/CPU)ì— ìë™ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
    torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì´ê³  ë¹ ë¥´ê²Œ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë°ì´í„° íƒ€ì…ì„ float16ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    revision="float16",         # float16ì„ ì§€ì›í•˜ëŠ” íŠ¹ì • ëª¨ë¸ ë¦¬ë¹„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
)
```


    tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]
    tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]
    tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]
    special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]
    config.json:   0%|          | 0.00/618 [00:00<?, ?B/s]
    model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]
    Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]
    model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]
    model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]
    Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
    generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]

```python
# ëª¨ë¸ì´ ë¨¸ì‹  ëŸ¬ë‹ì— ê´€í•œ ì‹œë¥¼ ì‘ì„±í•˜ë„ë¡ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
input_text = "Write me a poem about Machine Learning."

# ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ê³  í…ì„œë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤.
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda") # PyTorch í…ì„œë¡œ ë³€í™˜í•˜ê³  CUDA(GPU)ë¡œ ë³´ëƒ…ë‹ˆë‹¤.

# input_idsë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì—ì„œ í† í° ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
outputs = model.generate(**input_ids)

# ìƒì„±ëœ í† í°ì„ ë¬¸ìì—´ë¡œ ë””ì½”ë”©í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤.
print(tokenizer.decode(outputs[0]))
```

    /usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
      warnings.warn(


    <bos>Write me a poem about Machine Learning.
    In circuits of logic, a mind unseen,


ìœ„ ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ê°€ ì•„ë˜ ì²˜ëŸ¼ ì¶œë ¥ë˜ë‹¤ê°€ ì¤‘ë‹¨ë˜ì–´ ë‚˜ì˜µë‹ˆë‹¤.

```
<bos>Write me a poem about Machine Learning.
In circuits of logic, a mind unseen,
```

ê·¸ ì´ìœ ëŠ” model.generate() í•¨ìˆ˜ì—ì„œ max_lengthì˜ ê¸°ë³¸ê°’ì´ 20ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

- max_lengthë€ (ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ + ì¶œë ¥ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´)ì˜ í•©ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì¦‰ ì…ë ¥ê³¼ ì¶œë ¥ì„ ëª¨ë‘ ë”í•´ì„œ max_lengthì„ ë„˜ê¸°ì§€ ì•Šë„ë¡ ìƒì„±ë©ë‹ˆë‹¤.

ëª¨ë¸ì˜ ê²°ê³¼ê°€ ì¶©ë¶„íˆ ì‘ì„±ë  ìˆ˜ ìˆë„ë¡ max_lengthë¥¼ 512ë¡œ ì„¤ì •í•˜ì—¬ ë‹¤ì‹œ í˜¸ì¶œí•´ë´…ë‹ˆë‹¤.


```python
input_text = "Write me a poem about Machine Learning."
input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")

# input_idsë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì—ì„œ í† í° ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
# ìƒì„±ëœ ì‹œí€€ìŠ¤ëŠ” ì…ë ¥ ê¸¸ì´ë„ í¬í•¨í•˜ì—¬ ìµœëŒ€ 512 í† í° ê¸¸ì´ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
outputs = model.generate(**input_ids, max_length=512)
print(tokenizer.decode(outputs[0]))
```

    <bos>Write me a poem about Machine Learning.
    
    In circuits of logic, a mind unseen,
    A tapestry of data, a world serene.
    Algorithms dance, a symphony of code,
    Learning from experience, a journey bold.
    
    From images that speak, to numbers that bind,
    Machines learn, their knowledge entwined.
    Predictive models, with foresight so bright,
    Insights hidden, in the digital night.
    
    In self-driving cars, their sensors keen,
    Predicting paths, a seamless scene.
    Natural language processing, a bridge so wide,
    Understanding words, their meaning inside.
    
    But with power comes responsibility,
    Bias lurking, in the algorithms' abyss.
    Transparency elusive, a challenge to face,
    As machines learn, their consequences embrace.
    
    Yet, in this labyrinth of digital dreams,
    A future emerges, both wondrous and streams.
    A world where machines and humans unite,
    Creating a harmony, a technological delight.<eos>


ì •ìƒì ìœ¼ë¡œ ê²°ê³¼ê°€ ì¶œë ¥ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `<eos`> í† í°ì´ ë§ˆì§€ë§‰ì— ì¶œë ¥ë˜ëŠ”ë°ìš”. ì´ íŠ¹ìˆ˜í† í°ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

`<bos`>ì™€ `<eos`>ëŠ” íŠ¹ìˆ˜í•œ í† í°(token)ìœ¼ë¡œ, ê°ê° "beginning of sequence"ì™€ "end of sequence"ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ í† í°ë“¤ì€ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ë¬¸ì¥ì´ë‚˜ í…ìŠ¤íŠ¸ì˜ ì‹œì‘ê³¼ ëì„ ë‚˜íƒ€ë‚´ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê±°ë‚˜ í•´ì„í•  ë•Œ ì´ í† í°ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ êµ¬ë¶„í•˜ê²Œ ë©ë‹ˆë‹¤.

- `<bos`>: ë¬¸ì¥ì´ë‚˜ í…ìŠ¤íŠ¸ ì…ë ¥ì˜ ì‹œì‘ ë¶€ë¶„ì— ì‚¬ìš©ë˜ë©°, ëª¨ë¸ì´ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ì‹œì‘í•  ë•Œì˜ ì‹œì‘ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
- `<eos`>: ë¬¸ì¥ì´ë‚˜ í…ìŠ¤íŠ¸ ì…ë ¥ì˜ ë ë¶€ë¶„ì— ì‚¬ìš©ë˜ë©°, ëª¨ë¸ì´ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì¤‘ë‹¨í•´ì•¼ í•  ì‹œì ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ì´ëŸ¬í•œ í† í°ë“¤ì€ ëª¨ë¸ì´ ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ë¬¸ë§¥ì„ ë” ì˜ ì´í•´í•˜ê³ , ì ì ˆí•œ ê¸¸ì´ì˜ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ì—ì„œ `<eos`> í† í°ì´ ë“±ì¥í•˜ë©´, ëª¨ë¸ì€ ê·¸ ì‹œì ì—ì„œ ë¬¸ì¥ ìƒì„±ì„ ë©ˆì¶”ê³  ê·¸ê²ƒì„ ì¶œë ¥ì˜ ëìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.

### ì½”ë“œ ì‘ì„± í…ŒìŠ¤íŠ¸

ë¬¸ìì—´ì—ì„œ ê° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ì„¸ëŠ” íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•´ë‹¬ë¼ê³  ìš”ì²­í•´ë³´ê² ìŠµë‹ˆë‹¤. ë¬¸ìì—´ì€ ì•ì„œ ìƒì„±í–ˆë˜ ì‹œë¥¼ ì˜ˆì‹œë¡œ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.


```python
text = """
In circuits of logic, a mind unseen,
A tapestry of data, a world serene.
Algorithms dance, a symphony of code,
Learning from experience, a journey bold.
"""

input_text = f"Write a Python code to count the frequency of each word in the {text} and print the words in descending order of frequency."

input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")
outputs = model.generate(**input_ids, max_length=512)
print(tokenizer.decode(outputs[0]))
```

    <bos>Write a Python code to count the frequency of each word in the 
    In circuits of logic, a mind unseen,
    A tapestry of data, a world serene.
    Algorithms dance, a symphony of code,
    Learning from experience, a journey bold.
     and print the words in descending order of frequency.
    
    ```python
    from collections import Counter
    
    text = """
    In circuits of logic, a mind unseen,
    A tapestry of data, a world serene.
    Algorithms dance, a symphony of code,
    Learning from experience, a journey bold.
    """
    
    # Count the frequency of each word
    word_counts = Counter(text.split())
    
    # Sort the words in descending order of frequency
    sorted_words = word_counts.most_common(10)
    
    # Print the words
    for word, count in sorted_words:
        print(f"{word}: {count}")
    ```
    
    Output:
    
    ```
    Algorithms: 3
    code: 2
    data: 2
    journey: 2
    mind: 2
    process: 1
    sequence: 1
    tapestries: 1
    """
    ```<eos>


íŒŒì´ì¬ ì½”ë“œë¸”ë¡ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì´ì¬ ì½”ë“œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ ìƒì„±ëœ ì½”ë“œê°€ ì œëŒ€ë¡œ ë™ì‘í•˜ëŠ” ì§€ í…ŒìŠ¤íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤.




```python
from collections import Counter

text = """
In circuits of logic, a mind unseen,
A tapestry of data, a world serene.
Algorithms dance, a symphony of code,
Learning from experience, a journey bold.
"""

# Count the frequency of each word
word_counts = Counter(text.split())

# Sort the words in descending order of frequency
sorted_words = word_counts.most_common(10)

# Print the words
for word, count in sorted_words:
    print(f"{word}: {count}")
```

    a: 4
    of: 3
    In: 1
    circuits: 1
    logic,: 1
    mind: 1
    unseen,: 1
    A: 1
    tapestry: 1
    data,: 1


ì •ìƒì ìœ¼ë¡œ ì‘ë™ë©ë‹ˆë‹¤. ê·¸ëŸ¼ ì´ì œ ì „ì²´ ì‹œë¥¼ ì…ë ¥í•˜ê³  í…ŒìŠ¤íŠ¸ í•´ë³´ê² ìŠµë‹ˆë‹¤.


```python
from collections import Counter

text = """
In circuits of logic, a mind unseen,
A tapestry of data, a world serene.
Algorithms dance, a symphony of code,
Learning from experience, a journey bold.

From images that speak, to numbers that bind,
Machines learn, their knowledge entwined.
Predictive models, with foresight so bright,
Insights hidden, in the digital night.

In self-driving cars, their sensors keen,
Predicting paths, a seamless scene.
Natural language processing, a bridge so wide,
Understanding words, their meaning inside.

But with power comes responsibility,
Bias lurking, in the algorithms' abyss.
Transparency elusive, a challenge to face,
As machines learn, their consequences embrace.

Yet, in this labyrinth of digital dreams,
A future emerges, both wondrous and streams.
A world where machines and humans unite,
Creating a harmony, a technological delight.
"""

# Count the frequency of each word
word_counts = Counter(text.split())

# Sort the words in descending order of frequency
sorted_words = word_counts.most_common(10)

# Print the words
for word, count in sorted_words:
    print(f"{word}: {count}")
```

    a: 9
    of: 4
    their: 4
    A: 3
    in: 3
    In: 2
    world: 2
    that: 2
    to: 2
    learn,: 2


### ë§ˆë¬´ë¦¬

ì´ë²ˆì— ë¦´ë¦¬ì¦ˆëœ gemma 1.1 2b it ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸ í•´ë´¤ìŠµë‹ˆë‹¤. ì˜¨ë””ë°”ì´ìŠ¤ë‚˜ ê°„ë‹¨í•œ íƒœìŠ¤í¬ ì²˜ë¦¬ì— 2B ëª¨ë¸ì˜ í™œì•½ì´ ê¸°ëŒ€ë˜ëŠ” ë§Œí¼ ì•ìœ¼ë¡œ ì—¬ëŸ¬ê°€ì§€ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ë©´ì„œ í™œìš© ì‚¬ë¡€ë¥¼ ëŠ˜ë ¤ê°€ë³´ê² ìŠµë‹ˆë‹¤.

### ì¶”ê°€ë¬¸ì˜

* ì‘ì„±ì : ê¹€íƒœì˜
* ì´ë©”ì¼ : tykim@aifactory.page

### í•¨ê»˜ë³´ê¸°

* 1í¸ - Gemma ì‹œì‘í•˜ê¸° ë¹ ë¥¸ì‹¤í–‰ (ì¶”í›„ ê³µê°œ)
* [2í¸ - Gemma LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_lora_fine_tuning_fast_execute/)
* [3í¸ - Gemma í•œêµ­ì–´ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_korean_lora_fine_tuning_fast_execute/)
* [4í¸ - Gemma ì˜í•œë²ˆì—­ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_en2ko_lora_fine_tuning_fast_execute/)
* [5í¸ - Gemma í•œì˜ë²ˆì—­ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/22/gemma_ko2en_lora_fine_tuning_fast_execute/)
* [6í¸ - Gemma í•œêµ­ì–´ SQLì±—ë´‡ LoRA íŒŒì¸íŠœë‹ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/02/23/gemma_ko2sql_lora_fine_tuning_fast_execute/)
* [7í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì›¹ë¸Œë¼ìš°ì €í¸ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/04/02/gemma_ondevice_webbrowser_fast_execute/)
* [8í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì•„ì´í°(iOS)í¸ ë¹ ë¥¸ì‹¤í–‰](https://tykimos.github.io/2024/04/05/local_llm_installed_on_my_iphone_gemma_2b/)
* 9í¸ - Gemma ì˜¨ë””ë°”ì´ìŠ¤ íƒ‘ì¬ - ì•ˆë“œë¡œì´ë“œí¸ ë¹ ë¥¸ì‹¤í–‰ (ì‘ì—…ì¤‘)
* [10í¸ - RLHF íŠœë‹ìœ¼ë¡œ í–¥ìƒëœ Gemma 1.1 2B IT ê³µê°œ](https://tykimos.github.io/2024/04/08/rlhf_tuning_enhanced_gemma_1.1_2b_it_release/)